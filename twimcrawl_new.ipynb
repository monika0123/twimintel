{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twimcrawl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCN5zJkfY_eF",
        "colab_type": "code",
        "outputId": "98b037b8-11da-44f4-8422-1539db0242b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import bs4\n",
        "import re\n",
        "import datetime\n",
        "import lxml.etree as xml\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "\n",
        "url=[\"https://medium.com/the-banking-scene/the-future-of-banking-8b2c313f502a\",\"https://medium.com/job-automated/scrapping-medium-posts-using-scrapy-d5e8251dc008\"]\n",
        "\n",
        "def html_tag_remover(texts):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', texts)\n",
        "    return cleantext\n",
        "\n",
        "data=[]\n",
        "for i in url:\n",
        "  r=requests.get(i)\n",
        "  r.encoding = 'utf-8'\n",
        "  html_content = r.text\n",
        "  html_soup = BeautifulSoup(html_content, 'html.parser')\n",
        "  \n",
        "  para = html_soup.find_all('p',\"\")\n",
        "  topic= html_soup.find('h1').text\n",
        "  time = html_soup.find_all('a')\n",
        "  \n",
        "  \n",
        "  contest=html_tag_remover(str(para))\n",
        "  \n",
        "  top=html_tag_remover(str(topic))\n",
        " \n",
        "  year=html_tag_remover(str(time))\n",
        "  array = re.findall(r'\\d{4}', year) \n",
        "\n",
        " \n",
        "\n",
        "  query={   \"urls\" : i,\n",
        "            \"Contents\":contest,\n",
        "            \"topics\" : top,\n",
        "            \"year of publish\" :array,\n",
        "            \n",
        "           }\n",
        "  data.append(query)\n",
        "\n",
        "  dataframe=pd.DataFrame(data)\n",
        "  dataframe=dataframe.dropna()\n",
        "  print(dataframe)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                urls  ... year of publish\n",
            "0  https://medium.com/the-banking-scene/the-futur...  ...          [2019]\n",
            "\n",
            "[1 rows x 4 columns]\n",
            "                                                urls  ... year of publish\n",
            "0  https://medium.com/the-banking-scene/the-futur...  ...          [2019]\n",
            "1  https://medium.com/job-automated/scrapping-med...  ...          [2018]\n",
            "\n",
            "[2 rows x 4 columns]\n",
            "Drive already mounted at new_dataset.csv; to attempt to forcibly remount, call drive.mount(\"new_dataset.csv\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLrSGiwlp2F2",
        "colab_type": "code",
        "outputId": "1cf81bbb-b5f3-4cd8-81f8-86665d57acc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataframe[\"time\"]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Series([], Name: time, dtype: object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}